<html>
<head>
   <title>corvix GNU/Linux</title>
   <meta http-equiv="content-type" content="text/html; charset=latin1"/>
   <link href="style.css" rel=stylesheet type=text/css>
   <base target="main">
   <meta name="author" content="Gerolf Ziegenhain">
   <meta name="keywords" content="corvix linux gnu distribution debian egatrop">
   <link rel="shortcut icon" href="img/logo.ico" type="image/x-icon">
</head>
<body>

   
<h1>corvix GNU/Linux</h1>
Welcome to the webpage of the corvix distribution.
<br>
<b>Current status:</b> 
We are merging our working installation
into a reproduceable framework. 
This project is under heavy development
and <i>not yet working</i> for the end-user. 
Nevertheless you already may
check out our source tree or add the repositories to your current debian installation.
<font color=red>Warning: This is a fully atomatic installation, which will wipe your hdds!</font>

<h2>What is corvix?</h2>
Corvus corvax likes to steal; this distribution steals ideas :) 
It steals ideas in a positive way - we contribute a set of 
script, which you may use to build up your own high-performance
cluster without the need to search for all the packages / documentation
yourself. This distribution is the result of the lessons learned by building
up a 160+ CPU Linux cluster with several NFS servers from scratch.
<br>
In fact corvix is less a distribution and more a cheat sheet for installing 
debian in a scientific environment. After you install the
corvix distribution, you have a debian with additional
features.
<br>
Corvix GNU/Linux provides preconfigured profiles for both workstations 
(i.e. your laptop) and complete clusters. Nevertheless: If you expect
a colorful eyecandy system... forget it. Corvix is an advanced
distribution; or call it a dirty hack... It works for us ;)

<h2>Who may want to use this?</h2>
Nobody, except us! If you nevertheless want to use it, feel free to do so
without any kind of warranty.


<h2>Introduction</h2>
Probably you want to read this first, before you decide to try out
our stuff.

<h3>Motivation</h3>
Usually in a scientific environment one likes to have a stable 
base system (this also could be FreeBSD) and then only update 
a couple of really important packages to the bleeding edge.
Here, debian is chosen as the basis system. The corvix
distribution will install a straight debian/etch with a set
of common packages selected.
<br>
Any additional software will be compiled from scratch using
a portrage (Gentoo) alike system called <i>egatrop</i>.


<h3>Concept</h3>
The main concept is: <i>console rocks</i>.
<br>
There is no further documentation, than this file (which is included
in the distribution). For any changes / configuration you may want to 
do: RTFM.
<br>
<ul>
   <li>advanced user
   <li>textmode
   <li>full (?) documentation: download git source tree, and
      read <pre>/doc</pre>
</ul>


<h2>Package Management</h2>

<h3>Debian</h3>
For the stable packages we rely completely on the Debian GNU/Linux
distribution. If you want to use our stuff, you have to put the 
components in your /etc/apt/sources.list to use our repositories.
So far we have only a testing distribution (in terms of
debians vocabulary); once we have one 
complete framework, we will provide this as the stable distribution.

<h4>Key signatures:</h4>
We use secure apt. So use our public key to make sure, you apt-get no black-hat stuff.
This is the key:
<a href="http://pgpkeys.pca.dfn.de/pks/lookup?op=get&search=0x37A46DF5974E7D68">974E7D68</a>
(or here: <a href=http://corvix.eu/key/corvix.txt>key</a>)
with some 
<a href="http://pgpkeys.pca.dfn.de/pks/lookup?op=vindex&search=0x37A46DF5974E7D68&fingerprint=on">details</a>
and the fingerprint <i>0189 4F9A 5CFD 0242 5BEA  39E6 37A4 6DF5 974E 7D68</i>.
<ol>
<li>from keyserver
<pre>
gpg --keyserver pgpkeys.mit.edu --recv-key 974E7D68
gpg -a --export 974E7D68 | sudo apt-key add -
</pre>
<li>from keyring
<pre>
apt-get -y --force-yes install corvix-keyring
</pre>
</ol>


<h4>Component: meta</h4>
<pre>
deb http://corvix.eu testing meta 
</pre>
Contains virtual packages for quick configuration of lots of pcs.

<h4>Component: cluster</h4>
<pre>
deb http://corvix.eu testing cluster
</pre>
Contains cluster installation packages.

<h4>Component: ware</h4>
<pre>
deb http://corvix.eu testing ware
</pre>
Contains packages with more or less useful scripts/software.

<h4>Component: external</h4>
<pre>
deb http://corvix.eu testing external
</pre>
Contains some software, which is not included in the main debian
repositories. The packages have not been built by us.


<h2>Installation</h2>
<!--
The installation procedure is done over network, usb stick, cd or whatever. 
You may download
a bunch of scripts, which will build the bootable CD, USB stick
or setup a PXE boot server for you. If you want to save bandwidth
you may also choose to setup a mirror for the packages;
this can be a webserver, a CD or any other storage medium you
may access during the installation.
-->

<h3>Creating a primary bootimage</h3>
The easiest way is to start from a debian system. Check out our git source tree and then
create the bootimage in:
<br>
<pre>
cd var/install
./build
</pre>

<h4>Testing the bootimage</h4>
Once created you may test it using qemu.
At first create an empty disk image
<pre>
qemu-img create -f qcow ./corvix.qcow 10G
</pre>
Fire up qemu:
<pre>
qemu -cdrom boot/corvix.iso -hda corvix.qcow 
</pre>

<h3>Downloading a primary bootimage</h3>
You may also download a current <a href=http://corvix.eu/boot/corvix.iso>corvix.iso</a>.

<h3>Boot from cd</h3>
Burn the corvix.iso to cd :)

<h3>Boot from usb stick</h3>
Create a bootable stick:
<pre>
makebootstick /dev/YOURUSBDEVICE1234
</pre>
If you specify a partition (i.e. /dev/sdg2) the script will install the stuff in this partition
on the stick. And if you execute it with a whole disk (i.e. /dev/sdg) it will also create 
a patition layout on the stick.

<h3>PXE Network Bootserver</h3>
You need to have software:
<pre>
apt-get install dhcp3-server tftpd-hpa xinetd
</pre>
Insert some stuff in your /etc/dhcp3/dhcpd.conf 
<pre>
...
allow booting;
allow bootp;
filename "pxelinux.0";
...
</pre>
Configure xinetd
<pre>
# /etc/xinetd.d/tftp
service tftp
{
    protocol        = udp
    port            = 69
    socket_type     = dgram
    wait            = yes
    user            = root
    group           = root
    server          = /usr/sbin/in.tftpd 
    server_args = -s /var/lib/tftpboot/
    disable         = no
}
</pre>
Restart xinetd and copy the files 
<pre>
cp isoimage/* -rv /var/lib/tftpboot
chmod -R aog+rx /var/lib/tftpboot
</pre>


<h3>Arbitraty Debian Bootmedium</h3>
Just wait for the bootprompt and then type
<pre>
boot: auto url=corvix.eu 
</pre>
You may load classes:
<pre>
boot: auto url=corvix.eu classes=desktop
</pre>

<h3>Use existing debian installation</h3>
And add the repositories ;)


<!--
<h3>Features of the installer</h3>
Linux is about choice. Here, we restrict the features strictly
to some working configurations:
<ul>
   <li>Workstation
      <ul>
         <li>debootstrapped debian/etch
         <li>usual debian/etch bootcd
      </ul>
   <li>Cluster nodes:
      <ul>
         <li>Head server
         <li>Nodes: will install within under 4 minutes.
      </ul>
</ul>
Everything you want to do different: you can work as usual
with debian. For custom compiled stuff have a look at the
ebuilds.
-->


<h2>Corvix Source Tree</h2>

<h3>Offical releases</h3>
Full source tree of an official release.
<br>
(none yet)
   <!--<a href="http://gitweb.corvix.eu/corvix.git?a=snapshot;h=26ba9b9d6094f6abfe923cb34cecfe319e51d862;sf=tgz">v0.1.2</a>-->
   
<h3>Latest Snapshot</h3>
You may download the 
<a href="http://gitweb.corvix.eu/corvix.git?a=snapshot">Latest Snapshot</a>
via browser or clone the public repository.

<h3>GIT repository</h3>
<pre>
git clone git://git.corvix.eu/corvix.git
</pre>
The repositories are hosted on <a href=http://repo.or.cz>repo.or.cz</a>. 



<!--
<h2>Preconfigured installation profiles</h2>
We have to condense it... 

<h4>debian/etch</h4>

<h4>debian/etch preseef</h4>

<h4>corvix</h4>

<h4>corvix - cluster head server</h4>

<h4>corvix - cluster node</h4>

-->

<h2>Building a High-Performance Cluster</h2>

<h3>Planning</h3>

<h4>Choosing the hardware</h4>
In our case we want to run molecular dynamic dimulations; these systems are coupled
loosely and if we choose a good partition (which holds for any task, of course), we can
use cheap hardware to build our cluster.
<br>
Most computers avaliable today have two gigabit ports. As low-latency networking
is still much more costly (~same cost, as one additional node), we choose to
maximize the number of nodes instead. So our choice is dual-gigabit networking.
<br>
Another thing to consider is accessibility of the nodes. For a very small number of nodes,
it is not necessary to have them mounted in a rack. There are very comfortable choices
for terminal access (remote BIOS, KVM swichtes etc). We go the hard way, drop the
costs per node again and have only one screen and one keyboard to plug in; more
was never needed yet. Once the node tries to boot from hdd with pxeboot fallback,
there is no need for terminal access any more. And everything else will be done via
remote access (why should one want to stay in the loud server room?).

<h4>Structural Layout</h4>
We choose the following setup for the server overhead. Firstly we have a head server,
which will provide dhcp, dns, a debian mirror, nis, ganglia and the queue. Furthermore
this is the node, where the administrative stuff is done. It has two gigabit connections
to inside and one connection to outside.
<br>
Two identical login nodes provide user access; each has only one gigabit connection to inside
and one to outside.
<br>
A sum of three nas stations with hard-/software raid provide space. Each of them is connected
with two gigabit ports to the inside.

<h4>Dual Gigabit Networking</h4>
All nodes have two gigabit connections to inside. Each of them is connected in a different
network. These two different networks will decrease the network load.
The dual gigabit network is used in modulo2 mode. Every 2nd node will go through the 2nd
network to the nas stations; this way we circumvene the bottleneck at the nas without loosing
latency through channel bonding.
<br>
We can drop the costs even further by not using stackable switches. The only relevant drawback
will be, that one cannot use all nodes at once for one big task any more. By configuring
the partitions of nodes in the queue performance loss can be prevented.
<br>
Our cluster uses two 48 port and two 24 port switches. Both are linked, because the servers
then only once have to inject into each network. Two sets of nodes are now defined: one for
fast i/o to the nas stations and one for slow (link-bottleneck) i/o.

<h4>Network booting diskless clients</h4>
It may sound nice to have a complete root-over-nfs system booted via pxe. But the documentation
avaliable is very poor. Besides you may observe serious performance problems in booting more
then ten nodes at ones and even worse in running state.
<br>
Our conclusion is here: Use pxeboot to install the nodes automatically. A small local hdd
can also serve as local scratch for fast i/o.

<h4>Hardware Installation</h4>
riefly we summarize some lessons learned. If you choose to use different networks: use
different colors (we didn't). As with number of particles entropy will increase dramatically,
use cable straps and try to fix them at the rack asap. As hdds tend to break notoriously
consider using telescope mounts in a rack. It's also very useful to have spare hdds at hand.


<h4>Software Installation</h4>
We have chosen debian, because we thought it is the most stable distribution and easy
to maintain. After all it doesn't matter at all, what distribution you choose; all
new nodes will be installed from a golden node image using tar.
<br>
Following
the saying <i>If it ain't broke, don't fix it</i>: Our policy is to not change anything
after primary installation. In particular: don't update the distribution. We kept
a snapshot mirror of our distribution on the head server for future software installation.
<br>
From security point of view we have a locked rack with a trusted lan, so security doesn't
matter there. The login nodes have outwards connection and have the recent security
updates out course.
<br>
All important configuration files can be found in corvix. As in such a
cluster much stuff is done hardware specifically we can only provide the important
(portable) configuration files.


<h3>Configurations</h3>

<h4>All Configuration from DHCP</h4>
Our dhcp server supports pxeboot and provides hostnames to the known nodes. Because
we don't want to have redundant configurations, other configurations can be
created automatically from the dhcpd.conf.
<br>
This is how a configuration could look  alike
<pre>
#/etc/dhcp3/dhcpd.conf
allow booting;
allow bootp;

server-identifier earth.physik.uni-kl.de;
option domain-name "earth.physik.uni-kl.de";
authoritative;

option ntp-servers ntp.rhrk.uni-kl.de;

subnet 10.1.0.0 netmask 255.255.255.0 {
        range 10.1.0.200 10.1.0.240;
        filename "pxelinux.0";
}

group  {
        host s1 { hardware ethernet 00:30:1b:47:df:c9; fixed-address 10.1.0.101; }
        host s2 { hardware ethernet 00:30:1b:47:18:e6; fixed-address 10.1.0.102; }
        host s3 { hardware ethernet 00:30:1b:47:a6:d5; fixed-address 10.1.0.103; }
        host s4 { hardware ethernet 00:30:1b:47:f0:4e; fixed-address 10.1.0.104; }
        host s5 { hardware ethernet 00:30:1b:47:d6:04; fixed-address 10.1.0.105; }
        host s6 { hardware ethernet 00:30:1b:47:dd:33; fixed-address 10.1.0.106; }
        host s7 { hardware ethernet 00:30:1b:47:84:18; fixed-address 10.1.0.107; }
        host s8 { hardware ethernet 00:30:1b:47:16:54; fixed-address 10.1.0.108; }
        host s9 { hardware ethernet 00:30:1b:47:ef:2e; fixed-address 10.1.0.109; }
        host s10 { hardware ethernet 00:30:1b:47:17:6a; fixed-address 10.1.0.110; }
}
option routers 10.1.0.254;
option broadcast-address 10.1.0.255;

option domain-name-servers 10.1.0.254, 131.246.1.116, 131.246.9.116;

one-lease-per-client on;
default-lease-time 31104000;
max-lease-time 31104000;
use-host-decl-names on;

option ip-forwarding on;
</pre>

<h4>Queue system</h4>
We choose torque and in combination with the maui scheduler. Jobs may have low or high
qos and there is a standing reservation for quickshots in case of an overloaded queue.

<h4>SSH server</h4>
We use hostbased authentification inside the cluster and public key authentification
outside.

<h4>PXE Bootserver</h4>
Using the linux pxeboot, the head server provides debian network install and a custom
install script for adding new nodes in under 4 minutes automatically.


<h3>Further Software</h3>

<h4>Ganglia</h4>

<h4>Rgang</h4>

<h3>Our Cluster</h3>
Here is a list of the hardware we use (not mentioning the switches etc).
This cluster is property of the 
<a href="http://www.physik.uni-kl.de/urbassek/">Computational Material Science</a> group.

<table>
   <tr>
      <td>#</td>
      <td>Filing</td>
      <td>CPU</td>
      <td>Memory</td>
      <td>HDDs</td>
   </tr>
   <tr>
      <td>1</td>
      <td>Head server</td>
      <td>1x AMD Athlon DualCore 1.8GHz</td>
      <td>1GB</td>
      <td>80GB, 250GB Raid-0</td>
   </tr>
   <tr>
      <td>40</td>
      <td>Nodes</td>
      <td>2x AMD Opteron DualCore 2.4GHz</td>
      <td>8GB</td>
      <td>160GB</td>
   </tr>
   <tr>
      <td>1</td>
      <td>NAS</td>
      <td>1x AMD Opteron DualCore 2GHz</td>
      <td>4GB</td>
      <td>250GB, 4.1TB Raid-5</td>
   </tr>
   <tr>
      <td>1</td>
      <td>NAS</td>
      <td>1x Intel Pentium4 2.8GHz</td>
      <td>2GB</td>
      <td>80GB, 2.1TB Raid-5</td>
   </tr>
   <tr>
      <td>1</td>
      <td>NAS</td>
      <td>AMD Opteron DualCore 2GHz</td>
      <td>1GB</td>
      <td>250GB, 1.1TB Raid-5</td>
   </tr>
   <tr>
      <td>2</td>
      <td>Login</td>
      <td>Intel Pentium 4 2GHz</td>
      <td>1 GB</td>
      <td>250GB</td>
   </tr>
</table>

<h4>Some Pictures</h4>
<table>
   <tr align=top>
      <td><img src=img/Cluster_1stboot.jpg></td>
      <td><img src=img/Cluster_Networking.jpg></td>
      <td><img src=img/Cluster_Final.jpg></td>
      <td><img src=img/Cluster_Waste.jpg></td>
   </tr>
   <tr>
      <td>The cluster boots up for the 1st time</td>
      <td>The more cables, the more entropy you have to fight...</td>
      <td>Final cluster</td>
      <td>Drawback besides energy consumption: a lot of packing</td>
   </tr>
</table>


<!--
<h3>Profile: Head Server</h3>

<h3>Profile: NFS Servers</h2>

<h3>Profile: Nodes</h3>
-->

<h2>Legal Stuff</h2>
Of course we reject any kind of warranty. Expect your hardware to explode
and all your data to be deleted.
<br>
For the software snipplet, which are halfway included we have put the license
stuff under /doc/licenses. All software will be downloaded during the build
process and you are responsible on your own not to hurt any lawyers.


<div class=legal>
(C)opyleft G. Ziegenhain, Y. Rosandi 2008-2009<br>
Released under GPLv3 <a href="http://www.gnu.org">gnu.org</a>
</div>

</body>
</html>
