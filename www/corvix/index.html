<html>
<head>
   <title>corvix GNU/Linux</title>
   <meta http-equiv="content-type" content="text/html; charset=latin1"/>
   <link href="style.css" rel=stylesheet type=text/css>
   <base target="main">
   <meta name="author" content="Gerolf Ziegenhain">
   <meta name="keywords" content="corvix linux gnu distribution debian egatrop">
</head>
<body>

   
<h1>corvix GNU/Linux</h1>
Welcome to the webpage of the corvix distribution.
<br>
<b>Current status:</b> We are merging our working installation
into a reproduceable framework. This project is under heavy development
and <i>not yet working</i> for the end-user. Nevertheless you already may
check out our source tree.


<h2>What is corvix?</h2>

Corvus corvax likes to steal; this distribution steals ideas :) 
It steals ideas in a positive way - we contribute a set of 
script, which you may use to build up your own high-performance
cluster without the need to search for all the packages / documentation
yourself. This distribution is the result of the lessons learned by building
up a 160+ CPU Linux cluster with several NFS servers from scratch.
<br>
In fact corvix is less a distribution and more a cheat sheet for installing 
debian in a scientific environment. After you install the
corvix distribution, you have a debian/etch with additional
features.
<br>
Corvix GNU/Linux provides preconfigured profiles for both workstations 
(i.e. your laptop) and complete clusters. Nevertheless: If you expect
a colorful eyecandy system... forget it. Corvix is an advanced
distribution.


<h2>Introduction</h2>
Probably you want to read this first, before you decide to try out
our stuff.

<h3>Motivation</h3>
Usually in a scientific environment one likes to have a stable 
base system (this also could be FreeBSD) and then only update 
a couple of really important packages to the bleeding edge.
Here, debian is chosen as the basis system. The corvix
distribution will install a straight debian/etch with a set
of common packages selected.
<br>
Any additional software will be compiled from scratch using
a portrage (Gentoo) alike system called <i>egatrop</i>.


<h3>Concept</h3>
The main concept is: <i>console rocks</i>.
<br>
There is no further documentation, that this file (which is included
in the distribution). For any changes / configuration you may want to 
do: RTFM.
<br>
<ul>
   <li>advanced user
   <li>textmode
   <li>full documentation: download git source tree, and
      read <pre>/doc</pre>
</ul>


<h2>Package Management</h2>

<h3>Egatrop</h3>
The toolset egatrop has been developed. It provides
ebuilds, which do not check anything at all (like Gentoo ;)
and rely on the automake tools. You may better compare it to
slackware. Egatrop is completely independent on corvix.
You can compile all your custom stuff yourself with these
small cheatsheets with one command! All stuff resides
in /opt/egatrop.

<h4>Using egatrop separately</h4>
...


<h2>Installation</h2>
The installation procedure is done over network. You may download
a bunch of scripts, which will build the bootable CD, USB stick
or setup a PXE boot server for you. If you want to save bandwidth
you may also choose to setup a mirror for the packages;
this can be a webserver, a CD or any other storage medium you
may access during the installation.

<h3>Features of the installer</h3>
Linux is about choice. Here, we restrict the features strictly
to some working configurations:
<ul>
   <li>Workstation
      <ul>
         <li>debootstrapped debian/etch
         <li>usual debian/etch bootcd
      </ul>
   <li>Cluster nodes:
      <ul>
         <li>Head server
         <li>Nodes: will install within under 4 minutes.
      </ul>
</ul>
Everything you want to do different: you can work as usual
with debian. For custom compiled stuff have a look at the
ebuilds.


<h3>Obtaining the software</h3>
Download the source 

<h4>Corvix Bootstrap</h4>
<ul>
   <li>Offical releases<br>
      <a href="http://gitweb.corvix.eu/corvix.git?a=snapshot;h=26ba9b9d6094f6abfe923cb34cecfe319e51d862;sf=tgz">v0.1.2</a>
   <li>GIT repository<br>
      You may download the 
      <a href="http://gitweb.corvix.eu/corvix.git?a=snapshot">Latest Snapshot</a>
      via browser or clone the public repository
      <pre>
git clone git://git.corvix.eu/corvix.git
      </pre>
      The repositories are hosted on <a href=http://repo.or.cz>repo.or.cz</a>. 
</ul>

<h4>Egatrop</h4>
Egatrop can be downloaded seperately including ebuilds.
<ul>
   <li>Official releases<br>
   <li>GIT repositoriy<br>
      You may download the
      <a href="http://gitweb.corvix.eu/gitweb.cgi?p=egatrop;a=snapshot">Latest Snapshot</a>
      via browser.
   <li>Not yet public: git clone git.corvix.eu/git/egatrop
</ul>

<h4>Mirror</h4>
All software needed to build the source iso is mirrored here
<ul>
   <li><a href="http://mirror.corvix.eu">mirror.corvix.eu</a>
</ul>


<h3>Howto install?</h3>
...

<h4>Building the universal boot image</h4>
You have to build an universal boot image at first:
<pre>
cd lib/install
./make_image
</pre>
afterwards there will be a directory structure in <tt class=file>tmp/isoimage</tt>.
Now you can decide what boot method you'd like to use:
<ol>
   <li>Boot from CD<br>
      <pre>./makebootiso</pre>
      and the new boot image will reside in <tt class=file>~/corvix.iso</tt>.
      If you have <tt class=file>qemu</tt> installed you may test the image using
      <pre>./testbootiso</pre>
   <li>Boot from USB stick<br>
      <pre>./makebootstick /dev/sdWHATEVERYOURUSBSTICKIS</pre>
   <li>PXE boot server<br>

<h4>Mirror options</h4>
<ul>
   <li>local (nfs, hdd, cd)
   <li>mirror.corvix.eu
</ul>




<h3>Preconfigured installation profiles</h3>

<h4>debian/etch</h4>

<h4>debian/etch preseef</h4>

<h4>corvix</h4>

<h4>corvix - cluster head server</h4>

<h4>corvix - cluster node</h4>


<h2>High-Performance Cluster </h2>

<h3>Planning</h3>

<h4>Choosing the hardware</h4>
In our case we want to run molecular dynamic dimulations; these systems are coupled
loosely and if we choose a good partition (which holds for any task, of course), we can
use cheap hardware to build our cluster.
<br>
Most computers avaliable today have two gigabit ports. As low-latency networking
is still much more costly (~same cost, as one additional node), we choose to
maximize the number of nodes instead. So our choice is dual-gigabit networking.
<br>
Another thing to consider is accessibility of the nodes. For a very small number of nodes,
it is not necessary to have them mounted in a rack. There are very comfortable choices
for terminal access (remote BIOS, KVM swichtes etc). We go the hard way, drop the
costs per node again and have only one screen and one keyboard to plug in; more
was never needed yet. Once the node tries to boot from hdd with pxeboot fallback,
there is no need for terminal access any more. And everything else will be done via
remote access (why should one want to stay in the loud server room?).

<h4>Structural Layout</h4>
We choose the following setup for the server overhead. Firstly we have a head server,
which will provide dhcp, dns, a debian mirror, nis, ganglia and the queue. Furthermore
this is the node, where the administrative stuff is done. It has two gigabit connections
to inside and one connection to outside.
<br>
Two identical login nodes provide user access; each has only one gigabit connection to inside
and one to outside.
<br>
A sum of three nas stations with hard-/software raid provide space. Each of them is connected
with two gigabit ports to the inside.

<h4>Dual Gigabit Networking</h4>
All nodes have two gigabit connections to inside. Each of them is connected in a different
network. These two different networks will decrease the network load.
The dual gigabit network is used in modulo2 mode. Every 2nd node will go through the 2nd
network to the nas stations; this way we circumvene the bottleneck at the nas without loosing
latency through channel bonding.
<br>
We can drop the costs even further by not using stackable switches. The only relevant drawback
will be, that one cannot use all nodes at once for one big task any more. By configuring
the partitions of nodes in the queue performance loss can be prevented.
<br>
Our cluster uses two 48 port and two 24 port switches. Both are linked, because the servers
then only once have to inject into each network. Two sets of nodes are now defined: one for
fast i/o to the nas stations and one for slow (link-bottleneck) i/o.

<h4>Network booting diskless clients</h4>
It may sound nice to have a complete root-over-nfs system booted via pxe. But the documentation
avaliable is very poor. Besides you may observe serious performance problems in booting more
then ten nodes at ones and even worse in running state.
<br>
Our conclusion is here: Use pxeboot to install the nodes automatically. A small local hdd
can also serve as local scratch for fast i/o.

<h4>Hardware Installation</h4>
riefly we summarize some lessons learned. If you choose to use different networks: use
different colors (we didn't). As with number of particles entropy will increase dramatically,
use cable straps and try to fix them at the rack asap. As hdds tend to break notoriously
consider using telescope mounts in a rack. It's also very useful to have spare hdds at hand.


<h4>Software Installation</h4>
We have chosen debian, because we thought it is the most stable distribution and easy
to maintain. After all it doesn't matter at all, what distribution you choose; all
new nodes will be installed from a golden node image using tar.
<br>
Following
the saying <i>If it ain't broke, don't fix it</i>: Our policy is to not change anything
after primary installation. In particular: don't update the distribution. We kept
a snapshot mirror of our distribution on the head server for future software installation.
<br>
From security point of view we have a locked rack with a trusted lan, so security doesn't
matter there. The login nodes have outwards connection and have the recent security
updates out course.
<br>
All important configuration files can be found in corvix. As in such a
cluster much stuff is done hardware specifically we can only provide the important
(portable) configuration files.


<h3>Configurations</h3>

<h4>All Configuration from DHCP</h4>
Our dhcp server supports pxeboot and provides hostnames to the known nodes. Because
we don't want to have redundant configurations, other configurations can be
created automatically from the dhcpd.conf.

<h4>Queue system</h4>
We choose torque and in combination with the maui scheduler. Jobs may have low or high
qos and there is a standing reservation for quickshots in case of an overloaded queue.

<h4>SSH server</h4>
We use hostbased authentification inside the cluster and public key authentification
outside.

<h4>PXE Bootserver</h4>
Using the linux pxeboot, the head server provides debian network install and a custom
install script for adding new nodes in under 4 minutes automatically.


<h3>Further Software</h3>

<h4>Ganglia</h4>

<h4>Rgang</h4>

<h3>Our Cluster</h3>
Here is a list of the hardware we use (not mentioning the switches etc).
This cluster is property of the 
<a href="http://www.physik.uni-kl.de/urbassek/">Computational Material Science</a> group.

<table>
   <tr>
      <td>#</td>
      <td>Filing</td>
      <td>CPU</td>
      <td>Memory</td>
      <td>HDDs</td>
   </tr>
   <tr>
      <td>1</td>
      <td>Head server</td>
      <td>1x AMD Athlon DualCore 1.8GHz</td>
      <td>1GB</td>
      <td>80GB, 250GB Raid-0</td>
   </tr>
   <tr>
      <td>40</td>
      <td>Nodes</td>
      <td>2x AMD Opteron DualCore 2.4GHz</td>
      <td>8GB</td>
      <td>160GB</td>
   </tr>
   <tr>
      <td>1</td>
      <td>NAS</td>
      <td>1x AMD Opteron DualCore 2GHz</td>
      <td>4GB</td>
      <td>250GB, 4.1TB Raid-5</td>
   </tr>
   <tr>
      <td>1</td>
      <td>NAS</td>
      <td>1x Intel Pentium4 2.8GHz</td>
      <td>2GB</td>
      <td>80GB, 2.1TB Raid-5</td>
   </tr>
   <tr>
      <td>1</td>
      <td>NAS</td>
      <td>AMD Opteron DualCore 2GHz</td>
      <td>1GB</td>
      <td>250GB, 1.1TB Raid-5</td>
   </tr>
   <tr>
      <td>2</td>
      <td>Login</td>
      <td>Intel Pentium 4 2GHz</td>
      <td>1 GB</td>
      <td>250GB</td>
   </tr>
</table>

<h4>Some Pictures</h4>
<table>
   <tr align=top>
      <td><img src=img/Cluster_1stboot.jpg></td>
      <td><img src=img/Cluster_Networking.jpg></td>
      <td><img src=img/Cluster_Final.jpg></td>
      <td><img src=img/Cluster_Waste.jpg></td>
   </tr>
   <tr>
      <td>The cluster boots up for the 1st time</td>
      <td>The more cables, the more entropy you have to fight...</td>
      <td>Final cluster</td>
      <td>Drawback besides energy consumption: a lot of packing</td>
   </tr>
</table>


<h3>Profile: Head Server</h3>

<h3>Profile: NFS Servers</h2>

<h3>Profile: Nodes</h3>


<h2>Legal Stuff</h2>
Of course we reject any kind of warranty. Expect your hardware to explode
and all your data to be deleted.
<br>
For the software snipplet, which are halfway included we have put the license
stuff under /doc/licenses. All software will be downloaded during the build
process and you are responsible on your own not to hurt any lawyers.


<div class=legal>
(C)opyleft G. Ziegenhain, Y. Rosandi 2008<br>
Released under GPLv3 <a href="http://www.gnu.org">gnu.org</a>
</div>

</body>
</html>
